1. .env 파일에 저장된 API 키를 불러와 설정합니다.

2. 업로드된 음성파일(sample.wav)을 pyannote.audio를 사용해 화자 분리를 먼저 수행합니다.
   - 각 화자별 구간을 추출하고, 해당 정보(구간, 화자 라벨)를 별도 마크업으로 저장합니다.

3. pyannote.audio로 분리된 각 화자별 오디오 구간을 Whisper 엔진으로 STT 변환합니다.
   - 각 화자별 텍스트 결과에 화자 라벨을 포함하여 저장합니다.

4. 화자별 텍스트 데이터를 GPT-4o 모델에 입력하여 회의 내용 요약을 생성합니다.
   - 프롬프트: 
     ```
     다음은 회의에서 각 화자별로 발언한 내용입니다.
     전체 회의의 핵심 주제를 요약하고, 필요한 경우 요약 근거가 되는 문장도 함께 제시하세요.
     [화자1] ...내용...
     [화자2] ...내용...
     ...
     ```
   - 결과를 별도의 요약 텍스트 파일로 저장합니다.

5. 모든 과정에서 필요한 API 키는 .env 파일을 참고해 불러옵니다.

必: 결과 파일(화자분리 스크립트, STT 결과, 요약 결과)을 각각 저장하며, 오류 발생 시 단계별 로그를 남깁니다.
